Sun 18 Jan 2026 03:02:06 INFO  {'seed': 2020, 'dataset': 'Industrial_and_Scientific', 'bidirectional': False, 'n_heads': 2, 'lr': 0.0005, 'tau': 0.07, 'cl_weight': 0.4, 'mlm_weight': 0.2, 'neg_num': 25000, 'text_types': ['title', 'brand', 'features', 'categories', 'description'], 'epochs': 500, 'batch_size': 70, 'num_workers': 8, 'eval_step': 1, 'learner': 'AdamW', 'data_path': './dataset', 'map_path': '.emb_map.json', 'text_index_path': '.code.pq.20_256.pca128.title_brand_features_categories_description.json', 'text_emb_path': '.t5.meta.emb.npy', 'lr_scheduler_type': 'constant', 'gradient_accumulation_steps': 1, 'warmup_steps': 500, 'weight_decay': 0.0001, 'max_his_len': 100, 'n_codes_per_lel': 256, 'code_level': 20, 'early_stop': 100, 'embedding_size': 128, 'hidden_size': 512, 'n_layers': 2, 'n_layers_cross': 2, 'dropout_prob': 0.4, 'dropout_prob_cross': 0.1, 'mask_ratio': 0.5, 'device': 'cuda:1', 'metrics': 'recall@5,ndcg@5,recall@10,ndcg@10', 'valid_metric': 'ndcg@10', 'log_dir': './logs/Industrial_and_Scientific/分析_长度为100_原模型', 'ckpt_dir': './myckpt/', 'resume': None, 'run_local_time': 'Jan-18-2026_03-02', 'save_file_name': 'Jan-18-2026_03-02-f5c014_mlm0.2_cl0.4_maskratio0.5_drop0.4_dpcross0.1'}
Sun 18 Jan 2026 03:02:29 INFO  MGFSRec(
  (query_code_embedding): Embedding(5121, 128, padding_idx=0)
  (item_text_embedding): ModuleList(
    (0-4): 5 x Embedding(25849, 128, padding_idx=0)
  )
  (qformer): CrossAttTransformer(
    (layer): ModuleList(
      (0-1): 2 x CrossAttTransformerLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=128, out_features=128, bias=True)
          (key): Linear(in_features=128, out_features=128, bias=True)
          (value): Linear(in_features=128, out_features=128, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (out_linear): Linear(in_features=128, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.1, inplace=False)
        )
        (cross_attention): MultiHeadAttention(
          (query): Linear(in_features=128, out_features=128, bias=True)
          (key): Linear(in_features=128, out_features=128, bias=True)
          (value): Linear(in_features=128, out_features=128, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (out_linear): Linear(in_features=128, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (linear_1): Linear(in_features=128, out_features=512, bias=True)
          (linear_2): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (position_embedding): Embedding(100, 128)
  (transformer): Transformer(
    (layer): ModuleList(
      (0-1): 2 x TransformerLayer(
        (multi_head_attention): MultiHeadAttention(
          (query): Linear(in_features=128, out_features=128, bias=True)
          (key): Linear(in_features=128, out_features=128, bias=True)
          (value): Linear(in_features=128, out_features=128, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.4, inplace=False)
          (out_linear): Linear(in_features=128, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.4, inplace=False)
        )
        (feed_forward): FeedForward(
          (linear_1): Linear(in_features=128, out_features=512, bias=True)
          (linear_2): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.4, inplace=False)
        )
      )
    )
  )
  (dropout): Dropout(p=0.4, inplace=False)
  (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
  (loss_fct): CrossEntropyLoss()
)
